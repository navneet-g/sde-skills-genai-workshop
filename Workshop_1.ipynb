{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/navneet-g/sde-skills-genai-workshop/blob/main/Workshop_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnyqVd72Ole9"
      },
      "source": [
        "# Pre-reqs\n",
        "\n",
        "1. Bring a laptop or pair with someone to participate in hands-on lab\n",
        "2. Signup for free google Colab at https://colab.research.google.com/\n",
        "3. Get an api key at https://platform.openai.com/api-keys\n",
        "4. We will follow the Notebook at https://github.com/navneet-g/sde-skills-genai-workshop/blob/main/Workshop_1.ipynb\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FquK9rpp2iup"
      },
      "source": [
        "# Guess the word\n",
        "\n",
        "\n",
        "### Example 1\n",
        "* He loved to study and when he grew up he became a <?>\n",
        "\n",
        "### Example 2\n",
        "* He wore a white coat and stethoscope to work everyday. He loved to study and when he grew up he became a <?>.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "That is how transformers work, they guess the word based on the context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3PJLXmTJ4T-"
      },
      "source": [
        "* Large langugage models are trained on lots of data and can help and"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uoq1IKpG3BAv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5700250-382f-46e7-de9e-0a7a24ce6b1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m817.7/817.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m291.3/291.3 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.2/115.2 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m467.2/467.2 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.8/433.8 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install --q -U langchain langchain_community \\\n",
        "openai langchain-openai selenium unstructured \\\n",
        "langchain-text-splitters unstructured faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEqZ0VNsANo7",
        "outputId": "c0a1d93b-0ba9-4a0c-c0e2-bc5d87f3bef9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running locally: False\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import SeleniumURLLoader  # loading documents\n",
        "from langchain.text_splitter import CharacterTextSplitter  # splitting text\n",
        "from langchain_community.vectorstores import (\n",
        "    FAISS,\n",
        ")  # creating vector store from embeddings; can use chromadb instead as well\n",
        "from langchain.chains import RetrievalQA  # creating qa system\n",
        "from langchain_openai import ChatOpenAI  # using llm for qa system\n",
        "from langchain_openai import OpenAIEmbeddings  # embedding text with openai\n",
        "\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "from langchain_community.embeddings import OllamaEmbeddings\n",
        "\n",
        "import openai\n",
        "\n",
        "import importlib\n",
        "google_collab = importlib.util.find_spec('google')\n",
        "local_run = google_collab is None\n",
        "\n",
        "# try:\n",
        "#     imp.find_module('google')\n",
        "#     from google.colab import userdata\n",
        "#     local_run = False\n",
        "# except ImportError:\n",
        "#     local_run = True\n",
        "\n",
        "print(\"Running locally:\", local_run)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6QzKvO9LAaqT"
      },
      "outputs": [],
      "source": [
        "if local_run:\n",
        "  llm = ChatOllama(model=\"llama3:8b\")\n",
        "  embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
        "else:\n",
        "  from google.colab import userdata\n",
        "  openai_api_key=userdata.get('OPENAI_API_KEY')\n",
        "  llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\", openai_api_key=openai_api_key)\n",
        "  embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
        "\n",
        "\n",
        "# docs https://python.langchain.com/docs/integrations/llms/google_ai/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "233dT3HyAepr",
        "outputId": "e9c7ec02-af6e-4ad8-f7a0-74c81ddbcf65"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The last Academy Awards, also known as the Oscars, were held on April 25, 2021.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "llm.invoke(\"When were the last academy awards\").content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEy0dMD93JWL"
      },
      "source": [
        "# **In context learning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Fj0YwrH33Izp",
        "outputId": "1f788a36-4707-4721-ed95-2007efd8190d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The 96th Academy Awards took place on March 10, 2024.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "context = \"Context: \\n\\\n",
        "The 96th Academy Awards ceremony, which was presented by the Academy of Motion Picture Arts and Sciences (AMPAS), took place on March 10, 2024, at the Dolby Theatre in Hollywood, Los Angeles.[6] During the gala, the AMPAS presented Academy Awards (commonly referred to as Oscars) in 23 categories honoring films released in 2023. Comedian Jimmy Kimmel hosted the show for the fourth time.[a] \\n\"\n",
        "\n",
        "question = \"When were the 96th academy awards?\"\n",
        "\n",
        "llm.invoke(context + question).content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "UQMPYH1o4Ucj",
        "outputId": "7a8806ff-d9a6-4509-e0f7-7add173dc46c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I'm sorry, but I do not have access to real-time information, so I cannot provide you with the specific nominees for the 96th Academy Awards in 2024. I recommend checking the official Academy Awards website or a reliable news source for the most up-to-date information on the nominees.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "question =\"Who were the academy awards nominees\"\n",
        "llm.invoke(context + question).content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsQ1hTjT3Dyh"
      },
      "source": [
        "# **RAG**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bcsa1vXXAjjj",
        "outputId": "aacad1c9-a094-4653-a8ff-7bfb198757d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 7822, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1686, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1710, which is longer than the specified 1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Splitting document by character...\n",
            "Splitting into multiple documents...\n"
          ]
        }
      ],
      "source": [
        "# load url\n",
        "urls = [\n",
        "    \"https://en.wikipedia.org/wiki/96th_Academy_Awards\",\n",
        "]\n",
        "\n",
        "loader = SeleniumURLLoader(urls=urls)\n",
        "data = loader.load()\n",
        "\n",
        "\n",
        "# split document by character\n",
        "print(\"Splitting document by character...\")\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator=\"\\n\", chunk_size=1000, chunk_overlap=200\n",
        ")\n",
        "\n",
        "# split into multiple documents\n",
        "print(\"Splitting into multiple documents...\")\n",
        "docs = text_splitter.split_documents(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9wk_nAgAkLi",
        "outputId": "d08b4a52-3b1e-489b-b8f9-9596dca65b3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating vector store...\n"
          ]
        }
      ],
      "source": [
        "print(\"Creating vector store...\")\n",
        "# create vector store\n",
        "db = FAISS.from_documents(docs, embedding=embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bINlP9jcBJV7",
        "outputId": "9a99c721-4cd7-4367-eafc-e3f673b8b0be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating retriever...\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# create retriever to ask questions using openai and vector store\n",
        "print(\"Creating retriever...\")\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=db.as_retriever(search_kwargs={\"k\": 10}),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "64xmlqppAxE6"
      },
      "outputs": [],
      "source": [
        "def ask_question(question):\n",
        "    print(\"Query: \" + question)\n",
        "    print(\"Response: \" + qa.invoke(question)['result'])\n",
        "    print(\"===============\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByfZo3p-A0bj",
        "outputId": "0a348fbf-ea78-410e-d351-bb42eaa08d1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: Who were the academy awards winners?\n",
            "Response: The winners of the 96th Academy Awards included Oppenheimer, which won 7 awards, Poor Things, which won 4 awards, and The Zone of Interest, which won 2 awards. Specific categories and winners can be found in the full list of winners released by the Academy of Motion Picture Arts and Sciences.\n",
            "===============\n",
            "Query: What date did the academy awards happen?\n",
            "Response: The 96th Academy Awards ceremony took place on March 10, 2024, at the Dolby Theatre in Hollywood, Los Angeles.\n",
            "===============\n",
            "Query: What date did the 96th academy awards happen?\n",
            "Response: The 96th Academy Awards ceremony took place on March 10, 2024.\n",
            "===============\n"
          ]
        }
      ],
      "source": [
        "ask_question(\"Who were the academy awards winners?\")\n",
        "ask_question(\"What date did the academy awards happen?\")\n",
        "ask_question(\"What date did the 96th academy awards happen?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPBZ63njRJr8"
      },
      "source": [
        "## **Tools and Agents**\n",
        "\n",
        "Ref: [Building LLM Agents in 3 Levels of Complexity: From Scratch, OpenAI Functions & LangChain](https://www.youtube.com/watch?v=v1tyQtncsE4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "hCFeXVLsA6CX"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "class ModelWithTools:\n",
        "    def __init__(self, llm):\n",
        "        self.llm = llm\n",
        "\n",
        "    def get_response(self, prompt_question):\n",
        "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful research and programming assistant\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt_question}]\n",
        "        return llm.invoke(messages).content\n",
        "\n",
        "    def create_directory(self, directory_name):\n",
        "        subprocess.run([\"mkdir\", directory_name])\n",
        "\n",
        "    def create_file(self, file_name):\n",
        "        subprocess.run([\"touch\", file_name])\n",
        "\n",
        "    def list_files(self):\n",
        "        subprocess.run([\"ls\"])\n",
        "\n",
        "    def execute_function_call(self, function_call_string: str):\n",
        "        exec(function_call_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Fvh-Hw9P2G2c"
      },
      "outputs": [],
      "source": [
        "model = ModelWithTools(llm=llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWLeGwgE2G2c",
        "outputId": "3339a624-f47a-4080-e991-996ee7ed86c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model.create_directory('sde-skills-workshop')\n",
            "model.create_file('sde-skills-workshop/llm-agent-demo.md')\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "task_description = \"Create a folder called 'sde-skills-workshop'. Inside that folder, create a file called 'llm-agent-demo.md\"\n",
        "output = model.get_response(f\"\"\"Given a task that will be fed as input, and consider you have access to the following functions:\n",
        "\n",
        "    def create_directory(self, directory_name):\n",
        "        '''Function that creates a directory given a directory name.'''\n",
        "        subprocess.run([\"mkdir\", directory_name])\n",
        "\n",
        "    def create_file(self, file_name):\n",
        "        '''Function that creates a file given a file name.'''\n",
        "        subprocess.run([\"touch\", file_name])\n",
        "\n",
        "    def list_files(self):\n",
        "       '''Function that lists all files in the current directory.'''\n",
        "        subprocess.run([\"ls\"])\n",
        "\n",
        "    Your output should be the first function to be executed to complete the task containing the necessary arguments.\n",
        "    For example:\n",
        "\n",
        "    task: 'create a folder named lucas-the-agent-master'\n",
        "    output: model.create_directory('lucas-the-agent-master')\n",
        "\n",
        "    task: 'create a file named the-10-master-rules.md'\n",
        "    output: model.create_file('the-10-master-rules.md')\n",
        "\n",
        "    The OUTPUT SHOULD ONLY BE THE PYTHON FUNCTION CALL and NOTHING ELSE.\n",
        "    task: {task_description}\n",
        "    output:\\n\n",
        "    \"\"\")\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "p37z_KxM2G2d"
      },
      "outputs": [],
      "source": [
        "model.execute_function_call(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6THjcLwu2G2d",
        "outputId": "9958ff99-4a8f-46df-c8cc-9eeacd71d17f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Another copy of building-agents.ipynb Workshop-1.ipynb\n",
            "Copy of building-agents.ipynb         \u001b[34msde-skills-workdshop\u001b[m\u001b[m\n",
            "LangChain_RAG_GoogleAI.ipynb          \u001b[34msde-skills-workshop\u001b[m\u001b[m\n",
            "LangChain_RAG_OpenAI.ipynb\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bh4nWz9s2G2d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}