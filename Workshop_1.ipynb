{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/navneet-g/sde-skills-genai-workshop/blob/main/Workshop_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnyqVd72Ole9"
      },
      "source": [
        "# Pre-reqs\n",
        "\n",
        "1. Bring a laptop or pair with someone to participate in hands-on lab\n",
        "2. Signup for free google Colab at https://colab.research.google.com/\n",
        "3. Get an api key at https://platform.openai.com/api-keys\n",
        "4. We will follow the Notebook at https://github.com/navneet-g/sde-skills-genai-workshop/blob/main/Workshop_1.ipynb\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FquK9rpp2iup"
      },
      "source": [
        "# Guess the word\n",
        "\n",
        "\n",
        "### Example 1\n",
        "* He loved to study and when he grew up he became a <?>\n",
        "\n",
        "### Example 2\n",
        "* He wore a white coat and stethoscope to work everyday. He loved to study and when he grew up he became a <?>.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "That is how transformers work, they guess the word based on the context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3PJLXmTJ4T-"
      },
      "source": [
        "* Large langugage models are trained on lots of data and can help and"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoq1IKpG3BAv"
      },
      "outputs": [],
      "source": [
        "!pip install --q -U langchain langchain_community \\\n",
        "openai langchain-openai selenium unstructured \\\n",
        "langchain-text-splitters unstructured faiss-cpu langgraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEqZ0VNsANo7"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import SeleniumURLLoader  # loading documents\n",
        "from langchain.text_splitter import CharacterTextSplitter  # splitting text\n",
        "from langchain_community.vectorstores import (\n",
        "    FAISS,\n",
        ")  # creating vector store from embeddings; can use chromadb instead as well\n",
        "from langchain.chains import RetrievalQA  # creating qa system\n",
        "from langchain_openai import ChatOpenAI  # using llm for qa system\n",
        "from langchain_openai import OpenAIEmbeddings  # embedding text with openai\n",
        "\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "from langchain_community.embeddings import OllamaEmbeddings\n",
        "\n",
        "import openai\n",
        "\n",
        "import importlib\n",
        "google_collab = importlib.util.find_spec('google')\n",
        "local_run = google_collab is None\n",
        "\n",
        "# try:\n",
        "#     imp.find_module('google')\n",
        "#     from google.colab import userdata\n",
        "#     local_run = False\n",
        "# except ImportError:\n",
        "#     local_run = True\n",
        "\n",
        "print(\"Running locally:\", local_run)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QzKvO9LAaqT"
      },
      "outputs": [],
      "source": [
        "if local_run:\n",
        "  llm = ChatOllama(model=\"llama3:8b\")\n",
        "  embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
        "else:\n",
        "  from google.colab import userdata\n",
        "  openai_api_key=userdata.get('OPENAI_API_KEY')\n",
        "  llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\", openai_api_key=openai_api_key)\n",
        "  embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
        "\n",
        "\n",
        "# docs https://python.langchain.com/docs/integrations/llms/google_ai/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "233dT3HyAepr"
      },
      "outputs": [],
      "source": [
        "llm.invoke(\"When were the last academy awards before 2023\").content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEy0dMD93JWL"
      },
      "source": [
        "# **In context learning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fj0YwrH33Izp"
      },
      "outputs": [],
      "source": [
        "context = \"Context: \\n\\\n",
        "The 96th Academy Awards ceremony, which was presented by the Academy of Motion Picture Arts and Sciences (AMPAS), took place on March 10, 2024, at the Dolby Theatre in Hollywood, Los Angeles.[6] During the gala, the AMPAS presented Academy Awards (commonly referred to as Oscars) in 23 categories honoring films released in 2023. Comedian Jimmy Kimmel hosted the show for the fourth time.[a] \\n\"\n",
        "\n",
        "question = \"When were the 96th academy awards?\"\n",
        "\n",
        "llm.invoke(context + question).content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQMPYH1o4Ucj"
      },
      "outputs": [],
      "source": [
        "question =\"Who were the academy awards nominees\"\n",
        "llm.invoke(context + question).content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsQ1hTjT3Dyh"
      },
      "source": [
        "# **RAG**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bcsa1vXXAjjj"
      },
      "outputs": [],
      "source": [
        "# load url\n",
        "urls = [\n",
        "    \"https://en.wikipedia.org/wiki/96th_Academy_Awards\",\n",
        "]\n",
        "\n",
        "loader = SeleniumURLLoader(urls=urls)\n",
        "data = loader.load()\n",
        "\n",
        "\n",
        "# split document by character\n",
        "print(\"Splitting document by character...\")\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator=\"\\n\", chunk_size=1000, chunk_overlap=200\n",
        ")\n",
        "\n",
        "# split into multiple documents\n",
        "print(\"Splitting into multiple documents...\")\n",
        "docs = text_splitter.split_documents(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9wk_nAgAkLi"
      },
      "outputs": [],
      "source": [
        "print(\"Creating vector store...\")\n",
        "# create vector store\n",
        "db = FAISS.from_documents(docs, embedding=embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bINlP9jcBJV7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# create retriever to ask questions using openai and vector store\n",
        "print(\"Creating retriever...\")\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=db.as_retriever(search_kwargs={\"k\": 10}),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64xmlqppAxE6"
      },
      "outputs": [],
      "source": [
        "def ask_question(question):\n",
        "    print(\"Query: \" + question)\n",
        "    print(\"Response: \" + qa.invoke(question)['result'])\n",
        "    print(\"===============\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ByfZo3p-A0bj"
      },
      "outputs": [],
      "source": [
        "ask_question(\"Who were the academy awards winners?\")\n",
        "ask_question(\"What date did the academy awards happen?\")\n",
        "ask_question(\"What date did the 96th academy awards happen?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPBZ63njRJr8"
      },
      "source": [
        "## **Tools**\n",
        "\n",
        "Ref: [Building LLM Agents in 3 Levels of Complexity: From Scratch, OpenAI Functions & LangChain](https://www.youtube.com/watch?v=v1tyQtncsE4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCFeXVLsA6CX"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "class ModelWithTools:\n",
        "    def __init__(self, llm):\n",
        "        self.llm = llm\n",
        "\n",
        "    def get_response(self, prompt_question):\n",
        "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful research and programming assistant\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt_question}]\n",
        "        return llm.invoke(messages).content\n",
        "\n",
        "    def create_directory(self, directory_name):\n",
        "        subprocess.run([\"mkdir\", directory_name])\n",
        "\n",
        "    def create_file(self, file_name):\n",
        "        subprocess.run([\"touch\", file_name])\n",
        "\n",
        "    def list_files(self):\n",
        "        subprocess.run([\"ls\"])\n",
        "\n",
        "    def execute_function_call(self, function_call_string: str):\n",
        "        exec(function_call_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fvh-Hw9P2G2c"
      },
      "outputs": [],
      "source": [
        "model = ModelWithTools(llm=llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWLeGwgE2G2c"
      },
      "outputs": [],
      "source": [
        "from IPython.display import print\n",
        "\n",
        "task_description = \"Create a folder called 'sde-skills-workshop'. Inside that folder, create a file called 'llm-agent-demo.md\"\n",
        "output = model.get_response(f\"\"\"Given a task that will be fed as input, and consider you have access to the following functions:\n",
        "\n",
        "    def create_directory(self, directory_name):\n",
        "        '''Function that creates a directory given a directory name.'''\n",
        "        pass\n",
        "\n",
        "    def create_file(self, file_name):\n",
        "        '''Function that creates a file given a file name.'''\n",
        "        pass\n",
        "\n",
        "    def list_files(self):\n",
        "       '''Function that lists all files in the current directory.'''\n",
        "        pass\n",
        "\n",
        "    Your output should be the first function to be executed to complete the task containing the necessary arguments.\n",
        "    For example:\n",
        "\n",
        "    task: 'create a folder named lucas-the-agent-master'\n",
        "    output: model.create_directory('lucas-the-agent-master')\n",
        "\n",
        "    task: 'create a file named the-10-master-rules.md'\n",
        "    output: model.create_file('the-10-master-rules.md')\n",
        "\n",
        "    The OUTPUT SHOULD ONLY BE THE PYTHON FUNCTION CALL and NOTHING ELSE.\n",
        "    task: {task_description}\n",
        "    output:\\n\n",
        "    \"\"\")\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p37z_KxM2G2d"
      },
      "outputs": [],
      "source": [
        "model.execute_function_call(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IehrpoHa33-2"
      },
      "outputs": [],
      "source": [
        "# class Agent_Graph:\n",
        "#     def __init__(self):\n",
        "#         self.state = None\n",
        "#         self.nodes = {}\n",
        "#         self.edges = {}\n",
        "#         self.dynamic_edges = {}\n",
        "#         self.entry_point = None\n",
        "#         self.finish_point = None\n",
        "\n",
        "#     def add_node(self, node_id, node):\n",
        "#         self.nodes[node_id] = node\n",
        "\n",
        "#     def add_edge(self, node1_id, node2_id):\n",
        "#         if node1_id not in self.nodes:\n",
        "#             raise Exception( node1_id + \" does not exist\")\n",
        "#         if node2_id not in self.nodes:\n",
        "#             raise Exception(node2_id + \" does not exist\")\n",
        "\n",
        "#         if node1_id in self.edges:\n",
        "#             raise Exception (\"Edge already exists starting at \" + node1_id)\n",
        "\n",
        "#         self.edges[node1_id] = node2_id # node1_id -> node2_id\n",
        "\n",
        "#         # TODO: Check cycles later\n",
        "\n",
        "#     def add_dynamic_edge(self, node_1, condition):\n",
        "#         if node_1 not in self.nodes:\n",
        "#             raise Exception( node_1 + \" does not exist\")\n",
        "\n",
        "#         if node_1 in self.edges:\n",
        "#             raise Exception (\"Edge already exists starting at \" + node_1)\n",
        "\n",
        "#         if node_1 in self.dynamic_edges:\n",
        "#             raise Exception (\"Dynamic edge already exists starting at \" + node_1)\n",
        "\n",
        "#         self.edges[node_1] = None\n",
        "#         self.dynamic_edges[node_1] = condition\n",
        "\n",
        "\n",
        "#     def set_entry_point(self, node_id):\n",
        "#         if node_id not in self.nodes:\n",
        "#             raise Exception( node_id + \" does not exist\")\n",
        "#         self.entry_point = node_id\n",
        "\n",
        "#     def set_finish_point(self, node_id):\n",
        "#         if node_id not in self.nodes and node_id != \"STOP\":\n",
        "#             raise Exception( node_id + \" does not exist\")\n",
        "#         self.finish_point = node_id\n",
        "\n",
        "\n",
        "#     def stream(self, state):\n",
        "#         self.state = state\n",
        "#         if self.entry_point is None:\n",
        "#             raise Exception(\"No entry point\")\n",
        "#         if self.finish_point is None:\n",
        "#             raise Exception(\"No finish point\")\n",
        "\n",
        "#         current_node_id = self.entry_point\n",
        "#         while current_node_id != None and current_node_id != \"STOP\":\n",
        "#             node = self.nodes[current_node_id]\n",
        "#             self.state = node(self.state)\n",
        "#             yield (current_node_id, self.state)\n",
        "\n",
        "#             if(current_node_id == self.finish_point or current_node_id == \"STOP\"):\n",
        "#                 current_node_id = None\n",
        "#             else:\n",
        "#                 if current_node_id in self.dynamic_edges:\n",
        "#                     condition = self.dynamic_edges[current_node_id]\n",
        "#                     next_node_id = condition(self.state)\n",
        "\n",
        "#                     if(next_node_id not in self.nodes and next_node_id != \"STOP\"):\n",
        "#                         raise Exception(\"Next node does not exist: \", next_node_id)\n",
        "#                 else:\n",
        "#                     next_node_id  = self.edges[current_node_id]\n",
        "\n",
        "#                 current_node_id = next_node_id\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1g9ySQ1dfHP"
      },
      "source": [
        "# **Multi-step graphs**\n",
        "\n",
        "https://www.youtube.com/watch?v=R8KB-Zcynxc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6THjcLwu2G2d"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_animal_of_the_day(state):\n",
        "    print(\"Getting animal of the day...\")\n",
        "    animal = llm.invoke(\"Return a random animal as a central character of a story for kids. Return only the animal name nothing else otherwise I will charge you 1 million dollars.\").content\n",
        "    state['messages'].append(animal)\n",
        "    print(\"Animal of the day: \" + animal)\n",
        "    return state\n",
        "\n",
        "def get_place_of_the_day(state):\n",
        "    print(\"Getting place of the day...\")\n",
        "    messages = state['messages']\n",
        "    place = llm.invoke(\"Return a random place where the given animal can be in a story for kids. Return only the place name nothing else otherwise I will charge you 1 million dollars. The animal is: \" + messages[0]).content\n",
        "    state['messages'].append(place)\n",
        "    print(\"Place of the day: \" + place)\n",
        "    return state\n",
        "\n",
        "def get_mood_of_the_day(state):\n",
        "    print(\"Getting mood of the day...\")\n",
        "    messages = state['messages']\n",
        "    mood = llm.invoke(\"Return a random mood of an animal at the place to be used in the story. Return only the mood nothing else otherwise I will charge you 1 million dollars.  The animal is: \" + messages[0]+ \" and the place is: \" + messages[1]).content\n",
        "    state['messages'].append(mood)\n",
        "    print(\"Mood of the day: \" + mood)\n",
        "    return state\n",
        "\n",
        "def generate_story(state):\n",
        "    print(\"Generating story...\")\n",
        "    messages = state['messages']\n",
        "    animal = messages[0]\n",
        "    place = messages[1]\n",
        "    mood = messages[2]\n",
        "\n",
        "    story = llm.invoke(\"You are an expert in writing children's stories. Generate a story based on the animal, place and mood. The animal is: \" + animal + \" The place is: \" + place + \" The mood is: \" + mood).content\n",
        "\n",
        "    state['messages'].append(story)\n",
        "    print(\"Story: \" + story)\n",
        "    return state\n",
        "\n",
        "def validate_story(state):\n",
        "    print(\"Validating story...\")\n",
        "    print(\"State received: \", state)\n",
        "    messages = state['messages']\n",
        "    animal = messages[0]\n",
        "    place = messages[1]\n",
        "    mood = messages[2]\n",
        "    story = messages[-1]\n",
        "    result = llm.invoke(\"You are an expert in editing children's stories. Given the following animal name, place and mood verify that the story is based on the animal, place and mood. \\\n",
        "                        The animal is: \" + animal +\n",
        "                        \" The place is: \" + place +\n",
        "                        \" The mood is: \" + mood +\n",
        "                        \" The story is: \" + story +\n",
        "    \"If the story is not based on the animal, place and mood return 'continue' and if the story is based on the animal, place and mood return 'STOP'. Return only one word response 'generate_art' or 'STOP', nothing else otherwise I will charge you 1 million dollars.\").content\n",
        "\n",
        "    print(\"Validation result: \" + result)\n",
        "    return result\n",
        "\n",
        "def generate_art(state):\n",
        "    print(\"Generating art...\")\n",
        "    messages = state['messages']\n",
        "    story = messages[-1]\n",
        "    art = llm.invoke(\"You are an artist expert as creating an ascii art. Create an ascii art based on the story for kids. The story is: \" + story).content\n",
        "    messages.append(art)\n",
        "    return state\n",
        "\n",
        "from langgraph.graph import Graph, END\n",
        "\n",
        "graph = Graph()\n",
        "\n",
        "graph.add_node(\"get_animal_of_the_day\", get_animal_of_the_day)\n",
        "graph.add_node(\"get_place_of_the_day\", get_place_of_the_day)\n",
        "graph.add_node(\"get_mood_of_the_day\", get_mood_of_the_day)\n",
        "graph.add_node(\"generate_story\", generate_story)\n",
        "graph.add_node(\"generate_art\", generate_art)\n",
        "\n",
        "\n",
        "graph.add_edge(\"get_animal_of_the_day\", \"get_place_of_the_day\")\n",
        "graph.add_edge(\"get_place_of_the_day\", \"get_mood_of_the_day\")\n",
        "graph.add_edge(\"get_mood_of_the_day\", \"generate_story\")\n",
        "\n",
        "graph.add_conditional_edges(\"generate_story\", validate_story, {\n",
        "    \"generate_art\": \"generate_art\",\n",
        "    \"STOP\": END\n",
        "})\n",
        "\n",
        "\n",
        "\n",
        "graph.set_entry_point(\"get_animal_of_the_day\")\n",
        "graph.set_finish_point(\"generate_art\")\n",
        "\n",
        "app_state = {'messages': []}\n",
        "app = graph.compile()\n",
        "\n",
        "\n",
        "for output in app.stream(app_state):\n",
        "    # stream() yields dictionaries with output keyed by node name\n",
        "    for node_id, state in output.items():\n",
        "        print(f\"Output from node '{node_id}':\")\n",
        "        print(\"---\")\n",
        "        print(state)\n",
        "        print(\"\\n---\\n\")\n",
        "\n",
        "art = app_state['messages'][-1]\n",
        "story = app_state['messages'][-2]\n",
        "\n",
        "print(art)\n",
        "print(story)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaojj6Sr33-3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}